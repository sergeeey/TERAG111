# ü§ñ LLM Integration Guide

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è TERAG —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ LLM (Ollama, LM Studio)

## üìã –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã

### Ollama
- **URL:** `http://localhost:11434` (–∏–ª–∏ `http://host.docker.internal:11434` –∏–∑ Docker)
- **API:** `/api/generate`
- **–ú–æ–¥–µ–ª–∏:** llama3, mistral, codellama, –∏ –¥—Ä—É–≥–∏–µ

### LM Studio
- **URL:** `http://localhost:1234` (–∏–ª–∏ `http://host.docker.internal:1234` –∏–∑ Docker)
- **API:** OpenAI-compatible (`/v1/chat/completions`)
- **–ú–æ–¥–µ–ª–∏:** –õ—é–±—ã–µ –º–æ–¥–µ–ª–∏, –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –≤ LM Studio

## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

1. –°–∫–∞—á–∞–π—Ç–µ Ollama —Å https://ollama.ai
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ
3. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å:
   ```bash
   ollama pull llama3
   ```

### 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ LM Studio

1. –°–∫–∞—á–∞–π—Ç–µ LM Studio —Å https://lmstudio.ai
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ
3. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä (–ø–æ—Ä—Ç 1234)

### 3. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤ TERAG

–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ `config.env`:

#### –î–ª—è Ollama:
```env
LLM_PROVIDER=ollama
LLM_URL=http://host.docker.internal:11434
LLM_MODEL=llama3
```

#### –î–ª—è LM Studio:
```env
LLM_PROVIDER=lm_studio
LLM_URL=http://host.docker.internal:1234
LLM_MODEL=local-model
```

### 4. –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–æ–≤

```powershell
cd E:\TERAG
docker compose restart terag-api
```

## üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### API Endpoints

#### Get Context with LLM
```bash
curl -X POST http://localhost:8000/context \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is TERAG?",
    "use_llm": true,
    "temperature": 0.7,
    "max_tokens": 512
  }'
```

#### List Available Models
```bash
curl http://localhost:8000/llm/models
```

#### Check LLM Health
```bash
curl http://localhost:8000/llm/health
```

### Swagger UI

–û—Ç–∫—Ä–æ–π—Ç–µ http://localhost:8000/docs –∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã.

## üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã

### 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ LLM —Å–µ—Ä–≤–∏—Å–∞

```powershell
# –î–ª—è Ollama
curl http://localhost:11434/api/tags

# –î–ª—è LM Studio
curl http://localhost:1234/v1/models
```

### 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ TERAG API

```powershell
# Health check
curl http://localhost:8000/llm/health

# List models
curl http://localhost:8000/llm/models
```

### 3. –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

```powershell
curl -X POST http://localhost:8000/context `
  -H "Content-Type: application/json" `
  -d '{\"question\": \"Hello, who are you?\", \"use_llm\": true}'
```

## üêõ –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### –ü—Ä–æ–±–ª–µ–º–∞: LLM –Ω–µ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∏–∑ Docker

**–†–µ—à–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `host.docker.internal` –≤–º–µ—Å—Ç–æ `localhost`:
```env
LLM_URL=http://host.docker.internal:11434
```

### –ü—Ä–æ–±–ª–µ–º–∞: –¢–∞–π–º–∞—É—Ç—ã –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–∞—Ö

**–†–µ—à–µ–Ω–∏–µ:** –£–≤–µ–ª–∏—á—å—Ç–µ timeout –≤ `llm_client.py` –∏–ª–∏ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞

**–†–µ—à–µ–Ω–∏–µ:** –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:
```bash
# Ollama
ollama list

# LM Studio
curl http://localhost:1234/v1/models
```

### –ü—Ä–æ–±–ª–µ–º–∞: LLM –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—à–∏–±–∫–∏

**–†–µ—à–µ–Ω–∏–µ:** 
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: `docker compose logs terag-api`
2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ LLM —Å–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ—Ä—Ç—ã (–Ω–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –ª–∏ firewall)

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

LLM –º–µ—Ç—Ä–∏–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –≤ Prometheus –∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ Grafana:

- **Endpoint:** http://localhost:8000/metrics
- **Prometheus:** http://localhost:9090
- **Grafana:** http://localhost:3000

## üí° –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
```python
import requests

response = requests.post(
    "http://localhost:8000/context",
    json={
        "question": "Explain TERAG architecture",
        "use_llm": True
    }
)
print(response.json()["answer"])
```

### –° –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
```python
response = requests.post(
    "http://localhost:8000/context",
    json={
        "question": "What is knowledge graph?",
        "use_llm": True,
        "temperature": 0.5,  # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
        "max_tokens": 1024   # –ë–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
    }
)
```

### –ë–µ–∑ LLM (—Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç)
```python
response = requests.post(
    "http://localhost:8000/context",
    json={
        "question": "What is TERAG?",
        "use_llm": False  # –¢–æ–ª—å–∫–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    }
)
```

## üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [Ollama Documentation](https://github.com/ollama/ollama)
- [LM Studio Documentation](https://lmstudio.ai/docs)
- [TERAG API Documentation](http://localhost:8000/docs)

---

**–í–µ—Ä—Å–∏—è:** 1.0.0  
**–î–∞—Ç–∞:** 2025-01-27





















