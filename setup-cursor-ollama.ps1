# Cursor IDE + Ollama Setup
# Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Cursor Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¼ Ollama

Write-Host "Cursor IDE + Ollama Setup" -ForegroundColor Cyan
Write-Host "=========================" -ForegroundColor Cyan
Write-Host ""

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ollama
Write-Host "1. Checking Ollama..." -ForegroundColor Yellow
try {
    $response = Invoke-WebRequest -Uri "http://localhost:11434" -TimeoutSec 2 -ErrorAction Stop
    Write-Host "OK: Ollama is running" -ForegroundColor Green
} catch {
    Write-Host "ERROR: Ollama is not running. Starting Ollama..." -ForegroundColor Red
    Start-Process -FilePath "C:\Users\serge\AppData\Local\Programs\Ollama\ollama.exe" -ArgumentList "serve" -WindowStyle Minimized
    Start-Sleep -Seconds 3
    Write-Host "OK: Ollama started" -ForegroundColor Green
}

# ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
Write-Host "2. Available models:" -ForegroundColor Yellow
try {
    $modelsResponse = Invoke-WebRequest -Uri "http://localhost:11434/api/tags" -Method GET
    $modelsData = $modelsResponse.Content | ConvertFrom-Json
    $models = $modelsData.models | ForEach-Object { $_.name }
    
    Write-Host "Found models:" -ForegroundColor Green
    foreach ($model in $models) {
        Write-Host "  - $model" -ForegroundColor White
    }
} catch {
    Write-Host "WARNING: Could not fetch models list" -ForegroundColor Yellow
}

Write-Host ""

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ Cursor
Write-Host "3. Creating Cursor configuration..." -ForegroundColor Yellow

$cursorConfig = @{
    "cursor_ai_models" = @{
        "deepseek-coder" = @{
            "provider" = "openai"
            "model" = "deepseek-coder:6.7b"
            "baseURL" = "http://localhost:11434/v1"
            "apiKey" = "ollama"
            "description" = "DeepSeek Coder - ÐŸÑ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸ÑÑ‚ / Ð¸Ð½Ð¶ÐµÐ½ÐµÑ€Ð½Ñ‹Ð¹ Ð°Ð³ÐµÐ½Ñ‚"
        }
        "qwen2.5-instruct" = @{
            "provider" = "openai"
            "model" = "qwen2.5:7b-instruct"
            "baseURL" = "http://localhost:11434/v1"
            "apiKey" = "ollama"
            "description" = "Qwen2.5 - Ð¤Ð¸Ð»Ð¾ÑÐ¾Ñ„ / Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº-Ñ€Ð°ÑÑÑƒÐ¶Ð´Ð°Ñ‚ÐµÐ»ÑŒ"
        }
        "nous-hermes2" = @{
            "provider" = "openai"
            "model" = "nous-hermes2:10.7b"
            "baseURL" = "http://localhost:11434/v1"
            "apiKey" = "ollama"
            "description" = "Nous Hermes2 - Ð¡ÑƒÐ´ÑŒÑ / ÐºÐ¾Ð½ÑÐµÐ½ÑÑƒÑ-Ð°Ð³ÐµÐ½Ñ‚"
        }
        "mistral" = @{
            "provider" = "openai"
            "model" = "mistral:latest"
            "baseURL" = "http://localhost:11434/v1"
            "apiKey" = "ollama"
            "description" = "Mistral - ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚Ð¾Ñ€ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹"
        }
        "phi3" = @{
            "provider" = "openai"
            "model" = "phi3:latest"
            "baseURL" = "http://localhost:11434/v1"
            "apiKey" = "ollama"
            "description" = "Phi3 - Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÑ‚Ð¸Ðº"
        }
    }
    "mcp_servers" = @{
        "local_rag" = @{
            "name" = "local_rag"
            "command" = "python"
            "args" = @("ask_rag.py")
            "type" = "local"
            "description" = "RAG Ð¿Ð¾Ð¸ÑÐº Ð¿Ð¾ ÐºÐ¾Ð´Ð¾Ð²Ð¾Ð¹ Ð±Ð°Ð·Ðµ"
        }
        "quick_rag" = @{
            "name" = "quick_rag"
            "command" = "python"
            "args" = @("quick_rag.py")
            "type" = "local"
            "description" = "Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ RAG Ð¿Ð¾Ð¸ÑÐº"
        }
    }
    "workspace_settings" = @{
        "ai.model" = "deepseek-coder"
        "ai.enableCodeActions" = $true
        "ai.enableChat" = $true
        "ai.enableCompletions" = $true
        "ai.enableVoice" = $true
    }
}

# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ
$configPath = "cursor_ollama_config.json"
$cursorConfig | ConvertTo-Json -Depth 3 | Out-File -FilePath $configPath -Encoding UTF8
Write-Host "OK: Configuration saved to $configPath" -ForegroundColor Green

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸
$instructions = @"
# Cursor IDE + Ollama Setup Instructions

## ðŸŽ¯ Manual Setup Steps

### 1. AI Models Configuration
1. Open Cursor IDE
2. Go to Settings â†’ AI Models
3. Click "Add Custom Model"
4. Configure each model:

**DeepSeek Coder (Recommended for coding):**
- Provider: OpenAI
- Base URL: http://localhost:11434/v1
- API Key: ollama
- Model: deepseek-coder:6.7b

**Qwen2.5 (For reasoning):**
- Provider: OpenAI
- Base URL: http://localhost:11434/v1
- API Key: ollama
- Model: qwen2.5:7b-instruct

**Nous Hermes2 (For evaluation):**
- Provider: OpenAI
- Base URL: http://localhost:11434/v1
- API Key: ollama
- Model: nous-hermes2:10.7b

### 2. MCP Servers Configuration
1. Go to Settings â†’ MCP Servers
2. Click "Add Server"
3. Configure RAG servers:

**Local RAG:**
- Name: local_rag
- Command: python
- Args: ask_rag.py

**Quick RAG:**
- Name: quick_rag
- Command: python
- Args: quick_rag.py

### 3. Test the Setup
1. Open a chat in Cursor
2. Select "deepseek-coder" as your model
3. Ask: "Explain the loadGraph function in this codebase"
4. Use @local_rag to search the codebase

## ðŸš€ Quick Commands

# Test Ollama directly
curl http://localhost:11434

# Test RAG search
python quick_rag.py "loadGraph function"

# Launch model launcher
powershell -ExecutionPolicy Bypass -File .\ollama-launcher.ps1

## ðŸ“Š Model Roles

- **deepseek-coder:6.7b** - Best for coding tasks
- **qwen2.5:7b-instruct** - Best for reasoning and analysis
- **nous-hermes2:10.7b** - Best for evaluation and consensus
- **mistral:latest** - Best for general tasks
- **phi3:latest** - Fastest for quick responses

---
Ready for development! ðŸš€
"@

$instructions | Out-File -FilePath "CURSOR_OLLAMA_SETUP.md" -Encoding UTF8
Write-Host "OK: Instructions saved to CURSOR_OLLAMA_SETUP.md" -ForegroundColor Green

Write-Host ""
Write-Host "4. Testing setup..." -ForegroundColor Yellow

# Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ RAG
if (Test-Path "quick_rag.py") {
    Write-Host "Testing RAG search..." -ForegroundColor Gray
    python quick_rag.py "loadGraph function" 2>$null
    if ($LASTEXITCODE -eq 0) {
        Write-Host "OK: RAG search working" -ForegroundColor Green
    } else {
        Write-Host "WARNING: RAG search not working" -ForegroundColor Yellow
    }
} else {
    Write-Host "WARNING: quick_rag.py not found" -ForegroundColor Yellow
}

Write-Host ""
Write-Host "SETUP COMPLETE!" -ForegroundColor Green
Write-Host "===============" -ForegroundColor Green
Write-Host ""
Write-Host "Next steps:" -ForegroundColor Cyan
Write-Host "1. Open Cursor IDE" -ForegroundColor White
Write-Host "2. Follow instructions in CURSOR_OLLAMA_SETUP.md" -ForegroundColor White
Write-Host "3. Test with: python quick_rag.py 'loadGraph function'" -ForegroundColor White
Write-Host "4. Launch model launcher: powershell -ExecutionPolicy Bypass -File .\ollama-launcher.ps1" -ForegroundColor White
Write-Host ""
Write-Host "Ready for AI-powered development! ðŸš€" -ForegroundColor Green
