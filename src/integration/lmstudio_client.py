#!/usr/bin/env python3
"""
LM Studio Client –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ LLM inference-–±—ç–∫–µ–Ω–¥–∞ —Å TERAG
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç async –∑–∞–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ httpx –∏ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API
"""
import httpx
import logging
import time
from typing import Dict, Any, Optional, List
from datetime import datetime

logger = logging.getLogger(__name__)


class LMStudioError(Exception):
    """–ë–∞–∑–æ–≤–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ LM Studio"""
    pass


class ConnectionError(LMStudioError):
    """–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ LM Studio API"""
    pass


class TimeoutError(LMStudioError):
    """–û—à–∏–±–∫–∞ —Ç–∞–π–º–∞—É—Ç–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ LM Studio"""
    pass


class InvalidResponseError(LMStudioError):
    """–û—à–∏–±–∫–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LM Studio API"""
    pass


class LMStudioClient:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LM Studio API
    
    LM Studio –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API –Ω–∞ http://localhost:1234/v1
    """
    
    def __init__(
        self,
        base_url: str = "http://localhost:1234/v1",
        default_model: Optional[str] = None,
        timeout: float = 30.0
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LM Studio –∫–ª–∏–µ–Ω—Ç–∞
        
        Args:
            base_url: –ë–∞–∑–æ–≤—ã–π URL LM Studio API (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é http://localhost:1234/v1)
            default_model: –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –±—É–¥–µ—Ç –≤—ã–±—Ä–∞–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        self.base_url = base_url.rstrip('/')
        self.default_model = default_model
        self.timeout = timeout
        self._client: Optional[httpx.AsyncClient] = None
        
        logger.debug(f"LMStudioClient initialized: base_url={self.base_url}, model={self.default_model}")
    
    async def __aenter__(self):
        """Async context manager entry"""
        await self.connect()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.close()
    
    async def connect(self):
        """–°–æ–∑–¥–∞—Ç—å HTTP –∫–ª–∏–µ–Ω—Ç"""
        if self._client is None:
            self._client = httpx.AsyncClient(
                base_url=self.base_url,
                timeout=self.timeout,
                headers={"Content-Type": "application/json"}
            )
            logger.debug("LM Studio HTTP client created")
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å HTTP –∫–ª–∏–µ–Ω—Ç"""
        if self._client is not None:
            await self._client.aclose()
            self._client = None
            logger.debug("LM Studio HTTP client closed")
    
    def _chatml_to_prompt(self, messages: List[Dict[str, str]]) -> str:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç ChatML —Ñ–æ—Ä–º–∞—Ç –≤ plain prompt
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "user", "content": "..."}]
            
        Returns:
            Plain text prompt
        """
        prompt_parts = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                prompt_parts.append(f"System: {content}")
            elif role == "user":
                prompt_parts.append(f"User: {content}")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}")
        
        return "\n\n".join(prompt_parts)
    
    async def list_models(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π
            
        Raises:
            ConnectionError: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ API
            InvalidResponseError: –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç API –Ω–µ–≤–∞–ª–∏–¥–µ–Ω
        """
        await self.connect()
        
        start_time = time.time()
        
        try:
            logger.debug(f"Requesting models list from {self.base_url}/models")
            response = await self._client.get("/models")
            response.raise_for_status()
            
            data = response.json()
            latency = time.time() - start_time
            
            # OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {"data": [{"id": "model1", ...}, ...]}
            if "data" in data:
                models = [model.get("id", model.get("name", "")) for model in data["data"]]
            else:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                models = data.get("models", [])
                if isinstance(models, list) and models and isinstance(models[0], dict):
                    models = [m.get("id", m.get("name", "")) for m in models]
            
            models = [m for m in models if m]  # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ
            
            logger.info(f"Retrieved {len(models)} models in {latency:.2f}s")
            logger.debug(f"Models: {models}")
            
            return models
            
        except httpx.ConnectError as e:
            latency = time.time() - start_time
            logger.error(f"Connection error after {latency:.2f}s: {e}")
            raise ConnectionError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ LM Studio API: {e}")
        
        except httpx.TimeoutException as e:
            latency = time.time() - start_time
            logger.error(f"Timeout after {latency:.2f}s: {e}")
            raise TimeoutError(f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ LM Studio API: {e}")
        
        except httpx.HTTPStatusError as e:
            latency = time.time() - start_time
            logger.error(f"HTTP error {e.response.status_code} after {latency:.2f}s: {e}")
            raise InvalidResponseError(f"HTTP –æ—à–∏–±–∫–∞ {e.response.status_code}: {e}")
        
        except Exception as e:
            latency = time.time() - start_time
            logger.error(f"Unexpected error after {latency:.2f}s: {e}", exc_info=True)
            raise InvalidResponseError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
    
    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 512,
        messages: Optional[List[Dict[str, str]]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LM Studio API
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ—Å–ª–∏ messages –Ω–µ —É–∫–∞–∑–∞–Ω)
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è default_model –∏–ª–∏ –ø–µ—Ä–≤–∞—è –¥–æ—Å—Ç—É–ø–Ω–∞—è)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-2.0)
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ ChatML —Ñ–æ—Ä–º–∞—Ç–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞–¥ prompt)
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (top_p, frequency_penalty, etc.)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º:
            {
                "text": str,  # –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                "model": str,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
                "latency": float,  # –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
                "usage": dict,  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
                "finish_reason": str  # –ü—Ä–∏—á–∏–Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            }
            
        Raises:
            ConnectionError: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ API
            TimeoutError: –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—Ä–µ–≤—ã—Å–∏–ª —Ç–∞–π–º–∞—É—Ç
            InvalidResponseError: –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç API –Ω–µ–≤–∞–ª–∏–¥–µ–Ω
        """
        await self.connect()
        
        start_time = time.time()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å
        selected_model = model or self.default_model
        if not selected_model:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
            try:
                available_models = await self.list_models()
                if available_models:
                    selected_model = available_models[0]
                    logger.debug(f"Auto-selected model: {selected_model}")
                else:
                    raise InvalidResponseError("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ LM Studio")
            except Exception as e:
                logger.warning(f"Could not auto-select model: {e}")
                selected_model = "gpt-3.5-turbo"  # Fallback
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ payload
        if messages:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º ChatML —Ñ–æ—Ä–º–∞—Ç
            payload = {
                "model": selected_model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                **kwargs
            }
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º plain prompt (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ ChatML –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            payload = {
                "model": selected_model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": temperature,
                "max_tokens": max_tokens,
                **kwargs
            }
        
        try:
            logger.debug(f"Generating with model={selected_model}, temperature={temperature}, max_tokens={max_tokens}")
            response = await self._client.post("/chat/completions", json=payload)
            response.raise_for_status()
            
            data = response.json()
            latency = time.time() - start_time
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
            if "choices" in data and len(data["choices"]) > 0:
                choice = data["choices"][0]
                text = choice.get("message", {}).get("content", "")
                finish_reason = choice.get("finish_reason", "stop")
            else:
                raise InvalidResponseError("–û—Ç–≤–µ—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç choices")
            
            result = {
                "text": text,
                "model": selected_model,
                "latency": latency,
                "usage": data.get("usage", {}),
                "finish_reason": finish_reason,
                "raw_response": data  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
            }
            
            logger.info(f"Generated {len(text)} chars in {latency:.2f}s using {selected_model}")
            logger.debug(f"Finish reason: {finish_reason}, Usage: {result['usage']}")
            
            return result
            
        except httpx.ConnectError as e:
            latency = time.time() - start_time
            logger.error(f"Connection error after {latency:.2f}s: {e}")
            raise ConnectionError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ LM Studio API: {e}")
        
        except httpx.TimeoutException as e:
            latency = time.time() - start_time
            logger.error(f"Timeout after {latency:.2f}s: {e}")
            raise TimeoutError(f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ LM Studio API: {e}")
        
        except httpx.HTTPStatusError as e:
            latency = time.time() - start_time
            logger.error(f"HTTP error {e.response.status_code} after {latency:.2f}s: {e}")
            error_text = ""
            try:
                error_data = e.response.json()
                error_text = error_data.get("error", {}).get("message", str(e))
            except:
                error_text = str(e)
            raise InvalidResponseError(f"HTTP –æ—à–∏–±–∫–∞ {e.response.status_code}: {error_text}")
        
        except Exception as e:
            latency = time.time() - start_time
            logger.error(f"Unexpected error after {latency:.2f}s: {e}", exc_info=True)
            raise InvalidResponseError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
    
    async def health_check(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LM Studio API
        
        Returns:
            True –µ—Å–ª–∏ API –¥–æ—Å—Ç—É–ø–µ–Ω, False –∏–Ω–∞—á–µ
        """
        try:
            await self.connect()
            response = await self._client.get("/models", timeout=5.0)
            return response.status_code == 200
        except Exception as e:
            logger.debug(f"Health check failed: {e}")
            return False


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def example_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LMStudioClient"""
    async with LMStudioClient(base_url="http://localhost:1234/v1") as client:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        if await client.health_check():
            print("‚úÖ LM Studio –¥–æ—Å—Ç—É–ø–µ–Ω")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π
        models = await client.list_models()
        print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {models}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        result = await client.generate(
            prompt="Explain the concept of semantic graphs in AI.",
            temperature=0.7,
            max_tokens=256
        )
        print(f"üìù –û—Ç–≤–µ—Ç: {result['text']}")
        print(f"‚è±Ô∏è  Latency: {result['latency']:.2f}s")


if __name__ == "__main__":
    import asyncio
    asyncio.run(example_usage())













