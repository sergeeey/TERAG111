"""
OSINT Digest Generator
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç —Å —Ç–æ–ø —Å–∏–≥–Ω–∞–ª–∞–º–∏ –∏ —Ç—Ä–µ–Ω–¥–∞–º–∏
"""
import os
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from collections import Counter

logger = logging.getLogger(__name__)

try:
    from installer.app.modules.graph_updater import GraphUpdater
    GRAPH_AVAILABLE = True
except ImportError:
    GRAPH_AVAILABLE = False
    logger.warning("GraphUpdater not available, digest will use file-based sources only")


class OSINTDigestGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä OSINT-–¥–∞–π–¥–∂–µ—Å—Ç–∞ –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö –æ—Ç—á—ë—Ç–æ–≤"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.missions_dir = Path("missions")
        self.signals_file = self.data_dir / "weak_signals.json"
        self.discoveries_file = self.data_dir / "discoveries_report.md"
        self.graph_updater = None
        
        if GRAPH_AVAILABLE:
            try:
                self.graph_updater = GraphUpdater()
                if not self.graph_updater.driver:
                    self.graph_updater = None
            except Exception as e:
                logger.warning(f"Could not initialize GraphUpdater: {e}")
                self.graph_updater = None
    
    def get_recent_signals_from_graph(self, days: int = 7, limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–∏–≥–Ω–∞–ª—ã –∏–∑ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π"""
        if not self.graph_updater or not self.graph_updater.driver:
            return []
        
        try:
            with self.graph_updater.driver.session() as session:
                # –ó–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞ N –¥–Ω–µ–π
                query = """
                MATCH (s:Signal)
                WHERE s.discovered_at >= datetime() - duration({days: $days})
                RETURN s
                ORDER BY s.novelty_index DESC, s.confidence_ratio DESC
                LIMIT $limit
                """
                result = session.run(query, {"days": days, "limit": limit})
                
                signals = []
                for record in result:
                    node = record["s"]
                    signals.append({
                        "name": node.get("name", "Unknown"),
                        "novelty_index": node.get("novelty_index", 0.0),
                        "confidence": node.get("confidence_ratio", 0.0),
                        "source_url": node.get("source_url", ""),
                        "discovered_at": node.get("discovered_at", ""),
                        "description": node.get("description", "")
                    })
                
                return signals
        except Exception as e:
            logger.error(f"Error getting signals from graph: {e}")
            return []
    
    def get_recent_signals_from_files(self, days: int = 7, limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–∏–≥–Ω–∞–ª—ã –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        signals = []
        
        # –ß–∏—Ç–∞–µ–º weak_signals.json –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if self.signals_file.exists():
            try:
                with open(self.signals_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        signals.extend(data)
                    elif isinstance(data, dict) and "signals" in data:
                        signals.extend(data["signals"])
            except Exception as e:
                logger.warning(f"Could not read signals file: {e}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_signals = []
        
        for signal in signals:
            extracted_at = signal.get("extracted_at", signal.get("discovered_at", ""))
            if extracted_at:
                try:
                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç—ã
                    signal_date = None
                    try:
                        signal_date = datetime.fromisoformat(extracted_at.replace('Z', '+00:00'))
                    except:
                        try:
                            # –§–æ—Ä–º–∞—Ç –±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏
                            signal_date = datetime.strptime(extracted_at[:10], "%Y-%m-%d")
                        except:
                            pass
                    
                    if signal_date:
                        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–∞—Ç—É (–±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏)
                        if signal_date.date() >= cutoff_date.date():
                            recent_signals.append(signal)
                    else:
                        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–∫–ª—é—á–∞–µ–º (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–æ–≤—ã–º)
                        recent_signals.append(signal)
                except:
                    # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ, –≤–∫–ª—é—á–∞–µ–º —Å–∏–≥–Ω–∞–ª
                    recent_signals.append(signal)
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞—Ç—ã, –≤–∫–ª—é—á–∞–µ–º (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–æ–≤—ã–º)
                recent_signals.append(signal)
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –¥–∞—Ç–æ–π, –Ω–æ –µ—Å—Ç—å —Å–∏–≥–Ω–∞–ª—ã –≤–æ–æ–±—â–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ
        if not recent_signals and signals:
            recent_signals = signals
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ novelty_index –∏ confidence
        recent_signals.sort(
            key=lambda x: (
                x.get("novelty_index", 0.0),
                x.get("confidence", 0.0)
            ),
            reverse=True
        )
        
        return recent_signals[:limit]
    
    def get_top_signals(self, limit: int = 3) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø —Å–∏–≥–Ω–∞–ª—ã (–∏–∑ –≥—Ä–∞—Ñ–∞ –∏–ª–∏ —Ñ–∞–π–ª–æ–≤)"""
        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –≥—Ä–∞—Ñ–∞
        signals = self.get_recent_signals_from_graph(days=7, limit=limit * 2)
        
        # –ï—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –¥–æ–ø–æ–ª–Ω—è–µ–º –∏–∑ —Ñ–∞–π–ª–æ–≤
        if len(signals) < limit:
            file_signals = self.get_recent_signals_from_files(days=7, limit=limit * 2)
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º
            existing_names = {s.get("name", "") for s in signals}
            for sig in file_signals:
                if sig.get("name", "") not in existing_names:
                    signals.append(sig)
                    existing_names.add(sig.get("name", ""))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø
        signals.sort(
            key=lambda x: (
                x.get("novelty_index", 0.0),
                x.get("confidence", 0.0)
            ),
            reverse=True
        )
        
        return signals[:limit]
    
    def analyze_trends(self, signals: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏–≥–Ω–∞–ª–æ–≤"""
        if not signals:
            return {
                "trends": [],
                "domains": [],
                "avg_novelty": 0.0,
                "avg_confidence": 0.0
            }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–µ–Ω—ã –∏–∑ source_url
        domains = []
        for signal in signals:
            url = signal.get("source_url", "")
            if url:
                try:
                    from urllib.parse import urlparse
                    domain = urlparse(url).netloc
                    if domain:
                        domains.append(domain)
                except:
                    pass
        
        domain_counter = Counter(domains)
        top_domains = [domain for domain, _ in domain_counter.most_common(3)]
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        novelties = [s.get("novelty_index", 0.0) for s in signals if s.get("novelty_index")]
        confidences = [s.get("confidence", 0.0) for s in signals if s.get("confidence")]
        
        avg_novelty = sum(novelties) / len(novelties) if novelties else 0.0
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç—Ä–µ–Ω–¥—ã
        trends = []
        if avg_novelty > 0.8:
            trends.append("–í—ã—Å–æ–∫–∞—è –Ω–æ–≤–∏–∑–Ω–∞ —Å–∏–≥–Ω–∞–ª–æ–≤")
        if avg_confidence > 0.7:
            trends.append("–í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å")
        if len(signals) > 5:
            trends.append("–ê–∫—Ç–∏–≤–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è")
        
        return {
            "trends": trends,
            "domains": top_domains,
            "avg_novelty": avg_novelty,
            "avg_confidence": avg_confidence,
            "total_signals": len(signals)
        }
    
    def format_signal(self, signal: Dict[str, Any], index: int) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –¥–ª—è Telegram"""
        name = signal.get("name", "Unknown Signal")
        novelty = signal.get("novelty_index", 0.0)
        confidence = signal.get("confidence", 0.0)
        source = signal.get("source_url", "")
        description = signal.get("description", "")
        
        # –≠–º–æ–¥–∑–∏ –¥–ª—è —É—Ä–æ–≤–Ω—è –Ω–æ–≤–∏–∑–Ω—ã
        novelty_emoji = "üî•" if novelty > 0.8 else "‚≠ê" if novelty > 0.6 else "üí°"
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º
        lines = [f"{index}. {novelty_emoji} *{name}*"]
        
        if description:
            # –û–±—Ä–µ–∑–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–æ 150 —Å–∏–º–≤–æ–ª–æ–≤
            desc = description[:150] + "..." if len(description) > 150 else description
            lines.append(f"   {desc}")
        
        metrics = []
        if novelty > 0:
            metrics.append(f"–Ω–æ–≤–∏–∑–Ω–∞: {novelty:.2f}")
        if confidence > 0:
            metrics.append(f"—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}")
        
        if metrics:
            lines.append(f"   _{', '.join(metrics)}_")
        
        if source:
            # –û–±—Ä–µ–∑–∞–µ–º URL –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
            display_url = source.replace("https://", "").replace("http://", "")[:50]
            lines.append(f"   [–ò—Å—Ç–æ—á–Ω–∏–∫]({source})")
        
        return "\n".join(lines)
    
    def generate_digest(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞"""
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø —Å–∏–≥–Ω–∞–ª—ã
        top_signals = self.get_top_signals(limit=3)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–¥—ã
        trends_data = self.analyze_trends(top_signals)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–π–¥–∂–µ—Å—Ç
        lines = ["üîç *OSINT Digest*", ""]
        
        if not top_signals:
            lines.append("_–ù–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π._")
            return "\n".join(lines)
        
        # –¢–æ–ø —Å–∏–≥–Ω–∞–ª—ã
        lines.append("*–¢–æ–ø-3 –Ω–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–∞:*")
        lines.append("")
        
        for i, signal in enumerate(top_signals, 1):
            lines.append(self.format_signal(signal, i))
            lines.append("")
        
        # –¢—Ä–µ–Ω–¥—ã
        if trends_data["trends"]:
            lines.append("*–¢—Ä–µ–Ω–¥—ã:*")
            for trend in trends_data["trends"]:
                lines.append(f"‚Ä¢ {trend}")
            lines.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = []
        if trends_data["avg_novelty"] > 0:
            stats.append(f"–°—Ä–µ–¥–Ω—è—è –Ω–æ–≤–∏–∑–Ω–∞: {trends_data['avg_novelty']:.2f}")
        if trends_data["avg_confidence"] > 0:
            stats.append(f"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {trends_data['avg_confidence']:.2f}")
        if trends_data["total_signals"] > 0:
            stats.append(f"–í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {trends_data['total_signals']}")
        
        if stats:
            lines.append(f"*–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:* {', '.join(stats)}")
        
        # –¢–æ–ø –¥–æ–º–µ–Ω—ã
        if trends_data["domains"]:
            lines.append("")
            lines.append(f"*–¢–æ–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∏:* {', '.join(trends_data['domains'][:3])}")
        
        return "\n".join(lines)


def generate_daily_digest() -> str:
    """–£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞"""
    generator = OSINTDigestGenerator()
    return generator.generate_digest()

