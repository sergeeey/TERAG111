#!/usr/bin/env python3
"""
Document Converter for TERAG AI-REPS
–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Optional
import logging
from langdetect import detect

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def detect_language(text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞"""
    try:
        if len(text.strip()) < 10:
            return "unknown"
        lang = detect(text)
        return "ru" if lang == "ru" else "en"
    except Exception as e:
        logger.warning(f"Language detection failed: {e}")
        return "unknown"

def extract_text_from_docx(path: Path) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX —Ñ–∞–π–ª–æ–≤"""
    try:
        from docx import Document
        doc = Document(path)
        text_parts = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                text_parts.append(paragraph.text.strip())
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü
        for table in doc.tables:
            for row in table.rows:
                row_text = []
                for cell in row.cells:
                    if cell.text.strip():
                        row_text.append(cell.text.strip())
                if row_text:
                    text_parts.append(" | ".join(row_text))
        
        return "\n".join(text_parts)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX {path}: {e}")
        return ""

def extract_text_from_pdf(path: Path) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–æ–≤"""
    try:
        from PyPDF2 import PdfReader
        reader = PdfReader(path)
        text_parts = []
        
        for page_num, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text.strip():
                    text_parts.append(f"--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1} ---\n{page_text.strip()}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1} –≤ {path}: {e}")
                continue
        
        return "\n\n".join(text_parts)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF {path}: {e}")
        return ""

def extract_text_from_xlsx(path: Path) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ XLSX —Ñ–∞–π–ª–æ–≤"""
    try:
        import pandas as pd
        text_parts = []
        
        # –ß–∏—Ç–∞–µ–º –≤—Å–µ –ª–∏—Å—Ç—ã
        excel_file = pd.ExcelFile(path)
        for sheet_name in excel_file.sheet_names:
            try:
                df = pd.read_excel(path, sheet_name=sheet_name)
                if not df.empty:
                    text_parts.append(f"--- –õ–∏—Å—Ç: {sheet_name} ---")
                    text_parts.append(df.to_string(index=False, na_rep=''))
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ª–∏—Å—Ç–∞ {sheet_name} –≤ {path}: {e}")
                continue
        
        return "\n\n".join(text_parts)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ XLSX {path}: {e}")
        return ""

def extract_text_from_txt(path: Path) -> str:
    """–ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    except UnicodeDecodeError:
        try:
            with open(path, 'r', encoding='cp1251') as f:
                return f.read()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ {path}: {e}")
            return ""

def convert_all_to_txt(data_dir: str = "data") -> Path:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã"""
    data_path = Path(data_dir)
    txt_dir = data_path / "converted"
    txt_dir.mkdir(parents=True, exist_ok=True)
    
    # –ú–∞–ø–ø–∏–Ω–≥ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
    extractors = {
        ".docx": extract_text_from_docx,
        ".pdf": extract_text_from_pdf,
        ".xlsx": extract_text_from_xlsx,
        ".xls": extract_text_from_xlsx,  # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç Excel
        ".txt": extract_text_from_txt,
        ".md": extract_text_from_txt,
    }
    
    converted_count = 0
    error_count = 0
    
    logger.info(f"üîç –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ {data_path}")
    
    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏—â–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
    for ext, extractor in extractors.items():
        files = list(data_path.rglob(f"*{ext}"))
        logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º {ext}")
        
        for file_path in files:
            try:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                if "converted" in str(file_path):
                    continue
                
                logger.info(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {file_path.name}")
                text = extractor(file_path)
                
                if text.strip():
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞
                    language = detect_language(text)
                    
                    # –°–æ–∑–¥–∞—ë–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —è–∑—ã–∫–∞
                    output_name = f"{file_path.stem}_{language}.txt"
                    output_path = txt_dir / output_name
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç
                    with open(output_path, 'w', encoding='utf-8') as f:
                        f.write(f"# –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {file_path.name}\n")
                        f.write(f"# –ü—É—Ç—å: {file_path}\n")
                        f.write(f"# –†–∞–∑–º–µ—Ä: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤\n")
                        f.write(f"# –Ø–∑—ã–∫: {language}\n\n")
                        f.write(text)
                    
                    converted_count += 1
                    logger.info(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {file_path.name} ‚Üí {output_name} ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤, —è–∑—ã–∫: {language})")
                else:
                    logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ñ–∞–π–ª: {file_path.name}")
                    
            except Exception as e:
                error_count += 1
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path.name}: {e}")
    
    logger.info(f"üìä –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
    logger.info(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {converted_count} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"   ‚ùå –û—à–∏–±–æ–∫: {error_count} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"   üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç: {txt_dir}")
    
    return txt_dir

def get_converted_files(converted_dir: Path) -> List[Path]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    if not converted_dir.exists():
        return []
    
    txt_files = list(converted_dir.glob("*.txt"))
    logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(txt_files)} –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
    return txt_files

def analyze_document_structure(converted_dir: Path) -> Dict[str, any]:
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    txt_files = get_converted_files(converted_dir)
    
    if not txt_files:
        return {"error": "–ù–µ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"}
    
    analysis = {
        "total_files": len(txt_files),
        "total_size": 0,
        "file_sizes": [],
        "sources": set(),
        "years": set(),
        "file_types": {}
    }
    
    for file_path in txt_files:
        try:
            # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = file_path.stat().st_size
            analysis["total_size"] += file_size
            analysis["file_sizes"].append(file_size)
            
            # –ß–∏—Ç–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            with open(file_path, 'r', encoding='utf-8') as f:
                header = f.read(500)  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            if "–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª:" in header:
                source_line = [line for line in header.split('\n') if "–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª:" in line]
                if source_line:
                    source_file = source_line[0].split(": ")[1]
                    analysis["sources"].add(source_file)
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –≥–æ–¥ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                    import re
                    year_match = re.search(r'20\d{2}', source_file)
                    if year_match:
                        analysis["years"].add(year_match.group())
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if ".docx" in header:
                analysis["file_types"]["docx"] = analysis["file_types"].get("docx", 0) + 1
            elif ".pdf" in header:
                analysis["file_types"]["pdf"] = analysis["file_types"].get("pdf", 0) + 1
            elif ".xlsx" in header or ".xls" in header:
                analysis["file_types"]["excel"] = analysis["file_types"].get("excel", 0) + 1
            else:
                analysis["file_types"]["other"] = analysis["file_types"].get("other", 0) + 1
                
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {file_path}: {e}")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º set –≤ list –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    analysis["sources"] = list(analysis["sources"])
    analysis["years"] = list(analysis["years"])
    
    return analysis

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
    converted_dir = convert_all_to_txt()
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    analysis = analyze_document_structure(converted_dir)
    
    print("\n" + "="*50)
    print("üìä –ê–ù–ê–õ–ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í")
    print("="*50)
    print(f"üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {analysis.get('total_files', 0)}")
    print(f"üìè –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {analysis.get('total_size', 0) / 1024:.1f} KB")
    print(f"üìÖ –ì–æ–¥—ã: {', '.join(sorted(analysis.get('years', [])))}")
    print(f"üìÑ –¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤: {analysis.get('file_types', {})}")
    print(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {len(analysis.get('sources', []))} —Ñ–∞–π–ª–æ–≤")
    print("="*50)
