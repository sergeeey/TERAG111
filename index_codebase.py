#!/usr/bin/env python3
"""
üîç RAG Codebase Indexer
–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –∫–æ–¥ –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è RAG-–ø–æ–∏—Å–∫–∞
"""

import os
import sys
import argparse
from pathlib import Path
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
import json
import hashlib
from datetime import datetime

class CodebaseIndexer:
    def __init__(self, db_path: str = "chroma_db"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–µ—Ä–∞"""
        self.db_path = db_path
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_or_create_collection(
            name="codebase",
            metadata={"description": "TERAG Immersive Shell codebase"}
        )
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        self.code_extensions = {
            '.ts', '.tsx', '.js', '.jsx', '.py', '.md', '.json', 
            '.yml', '.yaml', '.toml', '.txt', '.css', '.scss'
        }
        
        # –ò—Å–∫–ª—é—á–∞–µ–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.exclude_dirs = {
            'node_modules', '.git', 'dist', 'build', '.next', 
            'chroma_db', '__pycache__', '.pytest_cache'
        }
    
    def should_index_file(self, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        if file_path.suffix not in self.code_extensions:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–∞–µ–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        for part in file_path.parts:
            if part in self.exclude_dirs:
                return False
        
        return True
    
    def read_file_content(self, file_path: Path) -> str:
        """–ß–∏—Ç–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='latin-1') as f:
                    return f.read()
            except Exception as e:
                print(f"WARNING: Cannot read {file_path}: {e}")
                return ""
        except Exception as e:
            print(f"WARNING: Error reading {file_path}: {e}")
            return ""
    
    def create_file_metadata(self, file_path: Path, content: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞—ë—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∞–π–ª–∞"""
        return {
            "file_path": str(file_path),
            "file_name": file_path.name,
            "file_extension": file_path.suffix,
            "file_size": len(content),
            "last_modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
            "content_hash": hashlib.md5(content.encode()).hexdigest(),
            "line_count": len(content.splitlines()),
            "language": self.detect_language(file_path)
        }
    
    def detect_language(self, file_path: Path) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é"""
        ext_to_lang = {
            '.ts': 'typescript',
            '.tsx': 'typescript',
            '.js': 'javascript',
            '.jsx': 'javascript',
            '.py': 'python',
            '.md': 'markdown',
            '.json': 'json',
            '.yml': 'yaml',
            '.yaml': 'yaml',
            '.css': 'css',
            '.scss': 'scss'
        }
        return ext_to_lang.get(file_path.suffix, 'text')
    
    def chunk_content(self, content: str, file_path: Path) -> List[Dict[str, Any]]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        lines = content.splitlines()
        chunks = []
        chunk_size = 50  # —Å—Ç—Ä–æ–∫ –Ω–∞ —á–∞–Ω–∫
        overlap = 10     # –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        
        for i in range(0, len(lines), chunk_size - overlap):
            chunk_lines = lines[i:i + chunk_size]
            chunk_content = '\n'.join(chunk_lines)
            
            if chunk_content.strip():
                chunks.append({
                    "content": chunk_content,
                    "start_line": i + 1,
                    "end_line": min(i + chunk_size, len(lines)),
                    "chunk_index": len(chunks)
                })
        
        return chunks
    
    def index_directory(self, directory: Path) -> Dict[str, int]:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é"""
        stats = {
            "files_processed": 0,
            "files_skipped": 0,
            "chunks_created": 0,
            "errors": 0
        }
        
        print(f"Indexing directory: {directory}")
        
        for file_path in directory.rglob("*"):
            if not file_path.is_file():
                continue
            
            if not self.should_index_file(file_path):
                stats["files_skipped"] += 1
                continue
            
            try:
                content = self.read_file_content(file_path)
                if not content:
                    stats["files_skipped"] += 1
                    continue
                
                # –°–æ–∑–¥–∞—ë–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata = self.create_file_metadata(file_path, content)
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                chunks = self.chunk_content(content, file_path)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
                for chunk in chunks:
                    chunk_id = f"{file_path.name}_{chunk['chunk_index']}"
                    chunk_metadata = {
                        **metadata,
                        "start_line": chunk["start_line"],
                        "end_line": chunk["end_line"],
                        "chunk_index": chunk["chunk_index"]
                    }
                    
                    self.collection.add(
                        documents=[chunk["content"]],
                        metadatas=[chunk_metadata],
                        ids=[chunk_id]
                    )
                    
                    stats["chunks_created"] += 1
                
                stats["files_processed"] += 1
                print(f"OK: Indexed: {file_path.relative_to(directory)} ({len(chunks)} chunks)")
                
            except Exception as e:
                print(f"ERROR: Error indexing {file_path}: {e}")
                stats["errors"] += 1
        
        return stats
    
    def create_index_summary(self, stats: Dict[str, int]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞—ë—Ç —Å–≤–æ–¥–∫—É –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        return {
            "indexed_at": datetime.now().isoformat(),
            "total_files_processed": stats["files_processed"],
            "total_files_skipped": stats["files_skipped"],
            "total_chunks_created": stats["chunks_created"],
            "total_errors": stats["errors"],
            "collection_size": self.collection.count()
        }

def main():
    parser = argparse.ArgumentParser(description="Index codebase for RAG search")
    parser.add_argument("--path", default=".", help="Path to codebase directory")
    parser.add_argument("--db-path", default="chroma_db", help="ChromaDB database path")
    parser.add_argument("--reset", action="store_true", help="Reset existing database")
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å
    codebase_path = Path(args.path)
    if not codebase_path.exists():
        print(f"‚ùå Path does not exist: {codebase_path}")
        sys.exit(1)
    
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –±–∞–∑—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if args.reset and Path(args.db_path).exists():
        import shutil
        shutil.rmtree(args.db_path)
        print(f"Reset database: {args.db_path}")
    
    # –°–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å–µ—Ä
    indexer = CodebaseIndexer(args.db_path)
    
    print("Starting codebase indexing...")
    print(f"Directory: {codebase_path.absolute()}")
    print(f"Database: {args.db_path}")
    print()
    
    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º
    stats = indexer.index_directory(codebase_path)
    
    # –°–æ–∑–¥–∞—ë–º —Å–≤–æ–¥–∫—É
    summary = indexer.create_index_summary(stats)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫—É
    summary_path = Path(args.db_path) / "index_summary.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print()
    print("INDEXING SUMMARY")
    print("==================")
    print(f"OK: Files processed: {stats['files_processed']}")
    print(f"SKIP: Files skipped: {stats['files_skipped']}")
    print(f"CHUNKS: Chunks created: {stats['chunks_created']}")
    print(f"ERRORS: Errors: {stats['errors']}")
    print(f"COLLECTION: Collection size: {summary['collection_size']}")
    print(f"SUMMARY: Summary saved: {summary_path}")
    
    if stats['errors'] == 0:
        print("\nSUCCESS: Indexing completed successfully!")
    else:
        print(f"\nWARNING: Indexing completed with {stats['errors']} errors")

if __name__ == "__main__":
    main()
