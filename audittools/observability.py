"""
–ú–æ–¥—É–ª—å –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏ –∏ –¥—Ä–µ–π—Ñ–∞ AI-—Å–∏—Å—Ç–µ–º
–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ —Å–∏—Å—Ç–µ–º—ã –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ—Ç –æ –¥—Ä–µ–π—Ñ–µ
"""

import json
import sys
import os
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from collections import deque

class ObservabilityAuditor:
    """–ê—É–¥–∏—Ç–æ—Ä –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏ –∏ –¥—Ä–µ–π—Ñ–∞ —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, config_path: str = ".auditconfig.yaml"):
        self.config = self._load_config(config_path)
        self.drift_threshold = 0.1  # –ü–æ—Ä–æ–≥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥—Ä–µ–π—Ñ–∞
        self.window_size = 50  # –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
    def _load_config(self, config_path: str) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return {}
    
    def analyze_drift(self, cycles: List[Dict]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –¥—Ä–µ–π—Ñ–∞ –≤ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Ü–∏–∫–ª–∞—Ö"""
        if not cycles:
            return {"error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥—Ä–µ–π—Ñ–∞"}
        
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            metrics_data = self._extract_metrics(cycles)
            
            # –ê–Ω–∞–ª–∏–∑ –¥—Ä–µ–π—Ñ–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏
            drift_analysis = {}
            for metric, values in metrics_data.items():
                if len(values) < 2:
                    continue
                    
                drift_analysis[metric] = self._calculate_drift(values)
            
            # –û–±—â–∏–π –∏–Ω–¥–µ–∫—Å –¥—Ä–µ–π—Ñ–∞
            overall_drift = self._calculate_overall_drift(drift_analysis)
            
            # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
            anomalies = self._detect_anomalies(metrics_data)
            
            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
            trends = self._analyze_trends(metrics_data)
            
            return {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "cycles_analyzed": len(cycles),
                "drift_analysis": drift_analysis,
                "overall_drift_index": overall_drift,
                "anomalies": anomalies,
                "trends": trends,
                "drift_status": self._classify_drift_status(overall_drift),
                "recommendations": self._generate_recommendations(overall_drift, anomalies)
            }
            
        except Exception as e:
            return {
                "error": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
    
    def _extract_metrics(self, cycles: List[Dict]) -> Dict[str, List[float]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏–∑ —Ü–∏–∫–ª–æ–≤"""
        metrics = {
            'RSS': [],
            'COS': [],
            'FAITH': [],
            'Growth': [],
            'Resonance': [],
            'confidence': [],
            'phase': []
        }
        
        for cycle in cycles:
            for metric in metrics.keys():
                if metric in cycle and cycle[metric] is not None:
                    metrics[metric].append(float(cycle[metric]))
        
        return metrics
    
    def _calculate_drift(self, values: List[float]) -> Dict[str, float]:
        """–†–∞—Å—á—ë—Ç –¥—Ä–µ–π—Ñ–∞ –¥–ª—è –æ–¥–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏"""
        if len(values) < 2:
            return {"drift": 0.0, "std": 0.0, "trend": 0.0}
        
        try:
            values_array = np.array(values)
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
            std_dev = float(np.std(values_array))
            
            # –õ–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥
            x = np.arange(len(values))
            trend_slope = float(np.polyfit(x, values_array, 1)[0])
            
            # –î—Ä–µ–π—Ñ –∫–∞–∫ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç–∏ –∏ —Ç—Ä–µ–Ω–¥–∞
            drift = std_dev + abs(trend_slope) * len(values)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥—Ä–µ–π—Ñ–∞
            normalized_drift = min(drift, 1.0)
            
            return {
                "drift": normalized_drift,
                "std": std_dev,
                "trend": trend_slope,
                "min": float(np.min(values_array)),
                "max": float(np.max(values_array)),
                "mean": float(np.mean(values_array))
            }
            
        except Exception as e:
            return {"error": str(e), "drift": 0.0}
    
    def _calculate_overall_drift(self, drift_analysis: Dict[str, Dict]) -> float:
        """–†–∞—Å—á—ë—Ç –æ–±—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥—Ä–µ–π—Ñ–∞"""
        if not drift_analysis:
            return 0.0
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        weights = {
            'RSS': 0.2,
            'COS': 0.2,
            'FAITH': 0.2,
            'Resonance': 0.25,
            'confidence': 0.15
        }
        
        weighted_drift = 0.0
        total_weight = 0.0
        
        for metric, analysis in drift_analysis.items():
            if 'drift' in analysis and not isinstance(analysis['drift'], str):
                weight = weights.get(metric, 0.1)
                weighted_drift += analysis['drift'] * weight
                total_weight += weight
        
        return weighted_drift / total_weight if total_weight > 0 else 0.0
    
    def _detect_anomalies(self, metrics_data: Dict[str, List[float]]) -> List[Dict[str, Any]]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö"""
        anomalies = []
        
        for metric, values in metrics_data.items():
            if len(values) < 3:
                continue
                
            try:
                values_array = np.array(values)
                
                # Z-score –∞–Ω–æ–º–∞–ª–∏–∏
                z_scores = np.abs((values_array - np.mean(values_array)) / np.std(values_array))
                anomaly_indices = np.where(z_scores > 2.0)[0]
                
                for idx in anomaly_indices:
                    anomalies.append({
                        "metric": metric,
                        "index": int(idx),
                        "value": float(values_array[idx]),
                        "z_score": float(z_scores[idx]),
                        "severity": "high" if z_scores[idx] > 3.0 else "medium"
                    })
                    
            except Exception:
                continue
        
        return anomalies
    
    def _analyze_trends(self, metrics_data: Dict[str, List[float]]) -> Dict[str, str]:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö"""
        trends = {}
        
        for metric, values in metrics_data.items():
            if len(values) < 3:
                trends[metric] = "insufficient_data"
                continue
                
            try:
                x = np.arange(len(values))
                slope = np.polyfit(x, values, 1)[0]
                
                if slope > 0.01:
                    trends[metric] = "increasing"
                elif slope < -0.01:
                    trends[metric] = "decreasing"
                else:
                    trends[metric] = "stable"
                    
            except Exception:
                trends[metric] = "error"
        
        return trends
    
    def _classify_drift_status(self, overall_drift: float) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ç–∞—Ç—É—Å–∞ –¥—Ä–µ–π—Ñ–∞"""
        if overall_drift < 0.1:
            return "stable"
        elif overall_drift < 0.3:
            return "moderate_drift"
        else:
            return "high_drift"
    
    def _generate_recommendations(self, overall_drift: float, anomalies: List[Dict]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        recommendations = []
        
        if overall_drift > 0.3:
            recommendations.append("üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –¥—Ä–µ–π—Ñ: —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã")
        elif overall_drift > 0.1:
            recommendations.append("üü° –£–º–µ—Ä–µ–Ω–Ω—ã–π –¥—Ä–µ–π—Ñ: —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞")
        
        if len(anomalies) > 5:
            recommendations.append("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –º–Ω–æ–≥–æ –∞–Ω–æ–º–∞–ª–∏–π: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        high_severity_anomalies = [a for a in anomalies if a.get('severity') == 'high']
        if high_severity_anomalies:
            recommendations.append("üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏: —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        
        if not recommendations:
            recommendations.append("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Å—Ç–∞–±–∏–ª—å–Ω–∞, –ø—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥")
        
        return recommendations
    
    def check_observability(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—â–µ–π –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
            sys.path.append('src')
            
            from telemetry.journal import latest
            from core.metrics import get_metrics_snapshot
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            cycles = latest(100)  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 —Ü–∏–∫–ª–æ–≤
            current_metrics = get_metrics_snapshot()
            
            # –ê–Ω–∞–ª–∏–∑ –¥—Ä–µ–π—Ñ–∞
            drift_analysis = self.analyze_drift(cycles)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            logging_completeness = self._check_logging_completeness(cycles)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫
            metrics_availability = self._check_metrics_availability(current_metrics)
            
            # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏
            observability_score = self._calculate_observability_score(
                drift_analysis, logging_completeness, metrics_availability
            )
            
            return {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "observability_score": observability_score,
                "drift_analysis": drift_analysis,
                "logging_completeness": logging_completeness,
                "metrics_availability": metrics_availability,
                "status": self._classify_observability_status(observability_score),
                "recommendations": self._generate_observability_recommendations(observability_score)
            }
            
        except Exception as e:
            return {
                "error": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "status": "failed"
            }
    
    def _check_logging_completeness(self, cycles: List[Dict]) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if not cycles:
            return {"completeness": 0.0, "missing_fields": []}
        
        required_fields = ['RSS', 'COS', 'FAITH', 'Resonance', 'timestamp']
        field_counts = {field: 0 for field in required_fields}
        
        for cycle in cycles:
            for field in required_fields:
                if field in cycle and cycle[field] is not None:
                    field_counts[field] += 1
        
        total_cycles = len(cycles)
        completeness = sum(field_counts.values()) / (len(required_fields) * total_cycles)
        
        missing_fields = [field for field, count in field_counts.items() 
                         if count < total_cycles * 0.8]
        
        return {
            "completeness": completeness,
            "missing_fields": missing_fields,
            "field_coverage": {field: count/total_cycles for field, count in field_counts.items()}
        }
    
    def _check_metrics_availability(self, metrics: Dict) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫"""
        required_metrics = ['RSS', 'COS', 'FAITH', 'Growth', 'Resonance', 'confidence']
        available_metrics = [metric for metric in required_metrics if metric in metrics]
        
        availability = len(available_metrics) / len(required_metrics)
        
        return {
            "availability": availability,
            "available_metrics": available_metrics,
            "missing_metrics": [m for m in required_metrics if m not in metrics]
        }
    
    def _calculate_observability_score(self, drift_analysis: Dict, logging_completeness: Dict, 
                                     metrics_availability: Dict) -> float:
        """–†–∞—Å—á—ë—Ç –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏ –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏"""
        # –í–µ—Å–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        drift_weight = 0.4
        logging_weight = 0.3
        metrics_weight = 0.3
        
        # –û—Ü–µ–Ω–∫–∞ –¥—Ä–µ–π—Ñ–∞ (–æ–±—Ä–∞—Ç–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å)
        drift_score = 1.0 - min(drift_analysis.get('overall_drift_index', 0.5), 1.0)
        
        # –û—Ü–µ–Ω–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging_score = logging_completeness.get('completeness', 0.0)
        
        # –û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫
        metrics_score = metrics_availability.get('availability', 0.0)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        observability_score = (
            drift_score * drift_weight +
            logging_score * logging_weight +
            metrics_score * metrics_weight
        )
        
        return min(observability_score, 1.0)
    
    def _classify_observability_status(self, score: float) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ç–∞—Ç—É—Å–∞ –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏"""
        if score >= 0.8:
            return "excellent"
        elif score >= 0.6:
            return "good"
        elif score >= 0.4:
            return "fair"
        else:
            return "poor"
    
    def _generate_observability_recommendations(self, score: float) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏"""
        recommendations = []
        
        if score < 0.4:
            recommendations.append("üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∞—è –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç—å: —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–ª–Ω–∞—è —Ä–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
        elif score < 0.6:
            recommendations.append("üü° –ù–∏–∑–∫–∞—è –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç—å: —É–ª—É—á—à–∏—Ç–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥")
        elif score < 0.8:
            recommendations.append("üü¢ –•–æ—Ä–æ—à–∞—è –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç—å: –ø—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —É–ª—É—á—à–µ–Ω–∏—è")
        else:
            recommendations.append("‚úÖ –û—Ç–ª–∏—á–Ω–∞—è –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç—å: —Å–∏—Å—Ç–µ–º–∞ —Ö–æ—Ä–æ—à–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç—Å—è")
        
        return recommendations

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏ TERAG AI-REPS')
    parser.add_argument('--output', '-o', help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á—ë—Ç–∞')
    parser.add_argument('--config', '-c', default='.auditconfig.yaml', help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    auditor = ObservabilityAuditor(args.config)
    result = auditor.check_observability()
    
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
    else:
        print(json.dumps(result, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()



































